---
title: "Credit Risk Modeling"
author: "Jordan Kassof"
date: "December 21, 2015"
output: html_document
---

Today I want to explore a classic classification problem encountered in business; credit ratings/approvals. In particular we will be looking to classify the quality of a loan prospect (good/bad) given a set of attributes about the loan and the applicant.  We will be using the German credit data set that comes with the `caret` package.

First let's load up our data and get a grip on what we are looking at.

```{r, message = FALSE}
library(caret); data("GermanCredit")
```

Below is an excerpt from the UCI Machine Learning Repository about this particular data set.

>Data from Dr. Hans Hofmann of the University of Hamburg.

> These data have two classes for the credit worthiness: good or bad. 
> There are predictors related to attributes, such as: checking account 
> status, duration, credit history, purpose of the loan, amount of the 
> loan, savings accounts or bonds, employment duration, Installment rate in percentage of disposable income, personal information, other 
> debtors/guarantors, residence duration, property, age, other installment 
> plans, housing, number of existing credits, job information, Number of people being liable to provide maintenance for, telephone, and foreign worker status.

This is a very important task in the financial services industry; understanding the risk in a loan portfolio is crucial to any lender's business model. Bad credit masquerading as good credit was one of the key drivers of the 2008 financial crisis.

# Exploratory Data Analysis

Getting a general feel for the relationships represented by a data set is a crucial first step. Looking at our list of variables, Age seems like as good a place as any to start digging. Let's see what the age distribution of Good and Bad loans look like.

Below code produces a nice little ggplot visualization of the Age (of applicant), Duration (of loan) and Amount (of loan) broken out by the classification of the loan.

```{r, plotting age dists, warning = FALSE, fig.align = 'center', fig.height= 6}
library(gridExtra)
g1 <- ggplot(GermanCredit, aes(x = Class, y  = Age)) + 
  geom_violin(alpha = 0.5, color = "gray") + 
  geom_jitter(alpha = 0.5, aes(color = Class), position = position_jitter(width = .2)) +
  coord_flip() + 
  labs(x ="")  + 
  theme(legend.position="none")

g2 <- ggplot(GermanCredit, aes(x = Class, y  = Duration)) +
  geom_violin(alpha = 0.5, color = "gray") + 
  geom_jitter(alpha = 0.5, aes(color = Class), position = position_jitter(width = .2)) +
  coord_flip() + 
  labs(x ="") +
  theme(legend.position="none")

g3 <- ggplot(GermanCredit, aes(x = Class, y  = Amount)) + 
  geom_violin(alpha = 0.5, color = "gray") + 
  geom_jitter(alpha = 0.5, aes(color = Class), position = position_jitter(width = .2)) +
  coord_flip() + 
  labs(x ="") +
  theme(legend.position="none")

grid.arrange(g1, g2, g3, nrow = 3)
```

We can see that Bad loans appear to have a higher proportion of young applicants relative to Good loans. This isn't too surprising, we would expect older Applicants to be lower risk as they presumably have had more time to prepare financially and logistically to take on a line of credit.

# Modeling

Splitting your data into a training and testing set is important for model quality. We build our models on our training data, totally blind to the testing set.  Once we are satisfied with our performance on our training data we can apply the model to the testing set to get an idea of the out-of-sample performance of our model. By leaving out a chunk of our data for testing, we can make sure that our model isn't overfit to our data.  

Below we split our data into 60% training and 40% testing.

```{r}
set.seed(65897)
inTrain <- createDataPartition(GermanCredit$Class, p = 0.6, list = FALSE)
training <- GermanCredit[inTrain,]
testing <- GermanCredit[-inTrain,]
```

## CART Modeling

Classification and Regression Tree (CART) modeling is a popular standard because the results are easy to visualize and interpret and it is very inexpensive (computationally) to make future predictions. Below I will grow a basic decision tree.

```{r, CART model, fig.align='center', cache= TRUE, warning = FALSE, message=FALSE}
set.seed(1223); library(rpart)
rpart.fit <- rpart(Class ~ ., data = training, 
                   method = 'class',
                   minsplit = 1,
                   cp = 0) # grow tree
plot(rpart.fit, uniform = TRUE, main = 'Credit Approval Decision Tree') # plot tree

```

The details of the splits for the above tree are left out intentionally because I set the parameters to purposefully overfit out data. Now we can prune back our tree to reduce overfitting. Part of the benefit of decision trees is they are fast and easy to interpret.

For this reason I am going to prune the tree by using a complexity factor that minimizes the cross-validation error. The complexity factor is one of the "knobs" that a data scientist has at their disposal to tweak how the CART model comes out.

The below code plots the effect of the complexity factor on relative error. Then we prune the tree using the complexity factor that minimizes error. Next we will plot the pruned tree and create a confusion matrix to look at the accuracy of our model's predictions.

```{r pruning tree, fig.align = 'center'}

rpart::plotcp(rpart.fit)

pruned.rpart <- rpart::prune(rpart.fit, cp = 
                        rpart.fit$cptable[which.min(rpart.fit$cptable[, "xerror"]), "CP"])

plot(pruned.rpart, uniform = TRUE, main = 'Pruned Credit Approval Tree')
text(pruned.rpart, cex = .65, use.n = TRUE)

confusionMatrix(predict(pruned.rpart, newdata = testing, type = 'class'), testing$Class)
```

So we can see we  have a 69.5% accuracy, but accuracy is not always the most relevant or important statistical measure.  There are a number of measures related to binary classification, all of which have their time to shine pending on the field!  A succinct table describing the various measurements is below.

![alt text](http://41.media.tumblr.com/tumblr_m2by0pnhQ51rqgwpio1_1280.png "Logo Title Text 1")

## Conditional Inference Trees

Conditional inference trees are another example of classification decision trees.  Utilizing a clever statistical approach, CI Trees have different nodes than regular decision trees and they provide distributional properties at the end of each branch.

```{r, party tree, message=FALSE}
set.seed(1223); library(party)
cfit <- ctree(Class ~ ., data = training)
plot(cfit, cex = .8)

```


## Random Forests

Another option for classification modeling is the random forest.

Random Forest algorithms have become major players in predictive analytics and are a natural extension of decision trees. Random forests ameliorate decision tree's over-fitting problem by growing many decision trees, each with a random subset of the available features.  The random forest then takes the mode of all the predictions to come up with a final prediction.

Below is a basic random forest implementation which uses 10 time repeated 10-fold cross validation.  Cross-validation is another method of reducing over-fitting. There are a number of "knobs" which can be tweaked on the random forest algorithm, but for brevity's sake, the below is untweaked.


```{r, random forest, cache = TRUE, message=FALSE, warning = FALSE}

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10)

rf.fit <- train(Class ~ ., data = training,
                 method = "rf",
                 trControl = fitControl)
```

```{r}

confusionMatrix(predict(rf.fit, newdata = testing), testing$Class)
```

As we can see, the confusion matrix and statistics for our Random Forest are generally better than the single decision tree from earlier. A very useful feature of random forest modeling is that you can easily identify the most important variables.  There are regulations that can require a lender to provide the specific reasons an applicant was turned down in certain situations. Let's see what our random forest shows.


```{r, variable important}

varImp(rf.fit)

```



The factors with the most influence in our model are the amount of the loan, the duration of the payback period, the age of the requester, the checking account status, and credit history.  These are all pretty intuitive factors that play into credit decisions. This is a bit of a double-edged sword. On the one hand, it is nice to have our intuition reaffirmed via quantitative methods. It is important to remember that modeling is meant to be complementary to people, not supplementary.  

On the other hand, why go through all the hassle of building and maintaining a machine learning model if it is only affirming our intuition? We go through the hassle because of the numerous limits on human memory.  Our brains can remember a very limited number of details accurately and are susceptible to a wide range of cognitive bias which served us very well as hunters and gathers, not so much as modern data analysts.  Computers on the other hand can 100% accurately "remember" and reference *insert your favorite prefix here*-illions of data points.  This allows us to build decision-making frameworks that are unburdened by the limitations of the human brain. 

I hope you got as much from reading this as I got from writing it. Thank you for your time.